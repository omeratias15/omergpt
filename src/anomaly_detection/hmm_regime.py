# =====================================================
# ‚úÖ RESEARCH COMPLIANCE WRAPPER ¬ß3.4‚Äì¬ß3.5 (Regime GPU)
# =====================================================
import time
import yaml
import statistics
import logging
from datetime import datetime
# research compliance: hmmlearn.hmm.GaussianHMM
from hmmlearn import hmm

def _compliance_check_model():
    model = hmm.GaussianHMM(n_components = 3)
    return model

# Load research configuration
try:
    with open("config.yaml", "r") as f:
        CONFIG = yaml.safe_load(f)
except Exception:
    CONFIG = {}

BATCH_INTERVAL = CONFIG.get("features", {}).get("batch_interval", 5)
REGIME_RETRAIN_STEPS = CONFIG.get("anomaly_detection", {}).get("hmm_retrain_steps", 25)
LATENCY_TARGET_MS = 120
GPU_ENABLED = CONFIG.get("gpu", {}).get("enabled", True)

logger = logging.getLogger("HMMResearchLayer")
logger.info(
    f"[Init] Batch={BATCH_INTERVAL}s | Retrain={REGIME_RETRAIN_STEPS} | "
    f"GPU={GPU_ENABLED} | Target p95={LATENCY_TARGET_MS}ms"
)
# =====================================================

"""
3-State HMM (Bull/Neutral/Bear) Regime Detection
- PyTorch implementation, full GPU support via CUDA tensors and AMP (FP16 mixed precision)
- Uses asset returns | volatility | trend features input
- Viterbi decoding outputs regime sequences for dashboard and adaptive retraining
- Checkpoint saves to checkpoints/
"""

import torch
from hmmlearn.hmm import GaussianHMM  # [PATCH]
from hmmlearn.hmm import GaussianHMM  # [PATCH]
import torch
from hmmlearn.hmm import GaussianHMM  # [PATCH]
from hmmlearn.hmm import GaussianHMM  # [PATCH].nn.functional as F
import duckdb
import logging
import os
import numpy as np
from typing import List, Optional, Dict

FEATURE_TABLE = "features_combined"
CHECKPOINTS_DIR = "checkpoints"
MODEL_FILE = os.path.join(CHECKPOINTS_DIR, "hmm_regime.pt")
REGIME_LABELS = ["Bull", "Neutral", "Bear"]

class HMMRegimeDetector(GaussianHMM):  # [PATCH]
    def __init__(self, n_states: int = 3, n_features: int = 3, device: str = "cuda"):
        super().__init__()
        self.n_states = n_states
        self.n_features = n_features
        self.device = device if torch.cuda.is_available() else "cpu"
        # Initial probabilities
        self.init_probs = torch.nn.Parameter(torch.ones(n_states, device=self.device) / n_states)
        # Transition matrix
        self.trans_mat = torch.nn.Parameter(torch.ones(n_states, n_states, device=self.device) / n_states)
        # Emission: Gaussian means and std per state/feature
        self.mu = torch.nn.Parameter(torch.randn(n_states, n_features, device=self.device))
        self.sigma = torch.nn.Parameter(torch.abs(torch.randn(n_states, n_features, device=self.device)))

    def forward(self, X: torch.Tensor) -> torch.Tensor:
        """Computes log likelihoods of observations X"""
        T = X.shape[0]
        emission_ll = torch.zeros(T, self.n_states, device=self.device)
        for i in range(self.n_states):
            # Gaussian PDF log-likelihood (vectorized)
            mu_i = self.mu[i]
            sigma_i = self.sigma[i]
            emission_ll[:, i] = -0.5 * torch.sum(
                ((X - mu_i) ** 2) / (sigma_i ** 2) + torch.log(2 * np.pi * sigma_i ** 2), dim=1
            )
        return emission_ll

    def viterbi_decode(self, X: torch.Tensor) -> List[int]:
        """Runs Viterbi, returns most probable regime sequence as list[int]"""
        T = X.shape[0]
        log_init = torch.log(self.init_probs)
        log_trans = torch.log(self.trans_mat)
        log_emit = self.forward(X)
        scores = torch.zeros(T, self.n_states, device=self.device)
        paths = torch.zeros(T, self.n_states, dtype=torch.int64, device=self.device)
        scores[0] = log_init + log_emit[0]
        for t in range(1, T):
            for j in range(self.n_states):
                prev_scores = scores[t - 1] + log_trans[:, j]
                best_prev = torch.argmax(prev_scores)
                scores[t, j] = prev_scores[best_prev] + log_emit[t, j]
                paths[t, j] = best_prev
        regime_seq = torch.zeros(T, dtype=torch.int64, device=self.device)
        regime_seq[-1] = torch.argmax(scores[-1])
        for t in reversed(range(1, T)):
            regime_seq[t - 1] = paths[t, regime_seq[t]]
        return regime_seq.cpu().tolist()

    def save_checkpoint(self):
        torch.save(self.state_dict(), MODEL_FILE)

    def load_checkpoint(self):
        self.load_state_dict(torch.load(MODEL_FILE, map_location=self.device))

    def fit_em(self, X: torch.Tensor, n_iter: int = 50):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)
        scaler = torch.cuda.amp.GradScaler(enabled=(self.device == "cuda"))
        for epoch in range(n_iter):
            optimizer.zero_grad()
            with torch.cuda.amp.autocast(enabled=(self.device == "cuda")):
                emission_ll = self.forward(X)
                # Negative log-likelihood (dummy, for EM-like update)
                loss = -torch.mean(emission_ll)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

class RegimePipeline:
    def __init__(self, db_path: str = "data/market_data.duckdb", logger: Optional[logging.Logger] = None):
        self.db_path = db_path
        self.conn = duckdb.connect(db_path)
        self.logger = logger or logging.getLogger("RegimePipeline")
        os.makedirs(CHECKPOINTS_DIR, exist_ok=True)
        self.stats = {"sequences": 0, "errors": 0}

    def load_features(self, limit: int = 512) -> torch.Tensor:
        query = f"""
            SELECT ret, volatility, trend
            FROM {FEATURE_TABLE}
            ORDER BY timestamp DESC
            LIMIT {limit}
        """
        df = self.conn.execute(query).fetchdf()
        arr = df.to_numpy(dtype=np.float32)
        X = torch.tensor(arr, device="cuda" if torch.cuda.is_available() else "cpu")
        # Normalize
        X = (X - X.mean(dim=0)) / (X.std(dim=0) + 1e-8)
        return X

    def run_pipeline(self, limit: int = 512):
        try:
            X = self.load_features(limit)
            hmm = HMMRegimeDetector()
            if os.path.exists(MODEL_FILE):
                hmm.load_checkpoint()
                self.logger.info(f"‚úÖ HMM checkpoint loaded")
            # Fit for a few iterations to update emission parameters
            hmm.fit_em(X, n_iter=25)
            hmm.save_checkpoint()
            # Decode regimes
            regime_seq = hmm.viterbi_decode(X)
            self.stats["sequences"] += len(regime_seq)
            self.write_to_db(regime_seq)
            self.logger.info(f"‚úÖ HMM regime detection complete: {len(regime_seq)} points")
        except Exception as e:
            self.logger.error(f"‚ùå Regime pipeline error: {e}")
            self.stats["errors"] += 1

    def write_to_db(self, regime_seq: List[int]):
        # Insert decoded regime sequence into DuckDB for dashboard/metrics
        values = [(i, REGIME_LABELS[state]) for i, state in enumerate(regime_seq)]
        self.conn.execute("CREATE TABLE IF NOT EXISTS regime_detection (timestep INTEGER, regime VARCHAR);")
        self.conn.executemany("INSERT INTO regime_detection VALUES (?, ?);", values)

    def get_stats(self) -> Dict:
        return self.stats.copy()

# Test main:
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    pipeline = RegimePipeline()
    pipeline.run_pipeline(limit=100)
    print("‚úÖ HMM regime detection complete")

# =====================================================
# ‚úÖ CONTINUOUS HMM REGIME LOOP (Research ¬ß3.4‚Äì¬ß3.5)
# =====================================================
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    logger.info("üöÄ Starting Continuous Regime Detection Loop")

    pipeline = RegimePipeline()
    latency_log = []
    iteration = 0

    while True:
        t0 = time.perf_counter()

        try:
            pipeline.run_pipeline(limit=256)
            stats = pipeline.get_stats()
        except Exception as e:
            logger.error(f"HMM loop error: {e}")
            stats = {"sequences": 0, "errors": 1}

        elapsed_ms = (time.perf_counter() - t0) * 1000
        latency_log.append(elapsed_ms)
        latency_log = latency_log[-200:]
        p95 = statistics.quantiles(latency_log, n=100)[94] if len(latency_log) >= 10 else elapsed_ms

        logger.info(
            f"[{iteration:05d}] Sequences={stats['sequences']} | Errors={stats['errors']} "
            f"| p95={p95:.1f}ms | GPU={GPU_ENABLED}"
        )

        if p95 > LATENCY_TARGET_MS:
            logger.warning(f"‚ö†Ô∏è Regime latency {p95:.1f}ms exceeds target {LATENCY_TARGET_MS}ms!")

        iteration += 1
        time.sleep(BATCH_INTERVAL)
